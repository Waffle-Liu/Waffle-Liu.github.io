<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Hongfu Liu's Homepage</title> <meta name="author" content="Hongfu Liu"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://waffle-liu.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <article> <div class="profile float-left"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/me2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/me2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/me2-1400.webp"></source> <img src="/assets/img/me2.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="me2.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="address"> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%6C%69%75.%68%6F%6E%67%66%75@%75.%6E%75%73.%65%64%75" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=6xFZDEcAAAAJ&amp;hl=zh-CN" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/Waffle-Liu" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/hongfu-liu-38585b184" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/waffle42567405" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a> </div> </div> <br> </div> <div class="clearfix"> <h1 class="post-title" style="font-size:1.6rem"> Hongfu Liu (刘洪甫) </h1> <p class="desc">Contact: liu (dot) hongfu [at] u (dot) nus (dot) edu</p> <p>Hi! I am a final-year Ph.D. student in <a href="https://smcnus.comp.nus.edu.sg/" target="_blank" rel="noopener noreferrer">SMC Lab</a> at <a href="https://www.nus.edu.sg/" target="_blank" rel="noopener noreferrer">National University of Singapore</a>. I am fortunate to be advised by Prof. <a href="https://www.comp.nus.edu.sg/cs/people/wangye/" target="_blank" rel="noopener noreferrer">Ye Wang</a>. Previously, I obtained my Bachelor’s degree in Computer Science at <a href="https://www.zju.edu.cn/english/" target="_blank" rel="noopener noreferrer">Zhejiang University</a>. </p> <p>I am broadly interested in machine learning for NLP and Multimodality topics. Currently, I focus more on improving LLMs and MLLMs from two directions:</p> <p>(1)<strong>Multimodal Reasoning</strong>: empowering LLMs with better language, speech, image, and video understanding and reasoning by designing data-efficient approaches;</p> <p>(2)<strong>Trustworthy</strong>: building trustworthy foundation models by investigating risks and enhancing the robustness and safety.</p> <p><strong><font color="red">I'm on the job market and looking for a Research Scientist/Engineer position. Feel free to reach out if you have any openings!</font></strong></p> </div> <br> <div class="news"> <h2 style="font-size:1.6rem">News</h2> <div class="table-responsive" style="max-height: 20vw"> <table class="table table-sm table-borderless "> <tr> <th scope="row">Jan, 2025</th> <td> Our paper “<strong>On Calibration of LLM-based Guard Models for Reliable Content Moderation</strong>” got accepted to <a href="https://iclr.cc/" target="_blank" rel="noopener noreferrer">ICLR 2025</a>! Many thanks to all co-authors! See you in Singapore <img class="emoji" title=":lion:" alt=":lion:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f981.png" height="20" width="20">! </td> </tr> <tr> <th scope="row">Dec, 2024</th> <td> Super excited to start my research internship at <a href="https://sail.sea.com/" target="_blank" rel="noopener noreferrer">Sea AI Lab (SAIL)</a>, working with Dr. <a href="https://p2333.github.io/" target="_blank" rel="noopener noreferrer">Tianyu Pang</a> and Dr. <a href="https://duchao0726.github.io/" target="_blank" rel="noopener noreferrer">Chao Du</a>! </td> </tr> <tr> <th scope="row">Sep, 2024</th> <td> Our paper “<strong>Advancing Adversarial Suffix Transfer Learning on Aligned Large Language Models</strong>” and “<strong>Advancing Test-Time Adaptation in Wild Acoustic Test Settings</strong>” got accepted to <a href="https://2024.emnlp.org/" target="_blank" rel="noopener noreferrer">EMNLP 2024</a> (two Main)! Many thanks to all my collaborators! Can’t wait to meet you in Miami! <img class="emoji" title=":sunny:" alt=":sunny:" src="https://github.githubassets.com/images/icons/emoji/unicode/2600.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Aug, 2024</th> <td> Our paper “<strong>Discursive Socratic Questioning: Evaluating the Faithfulness of Language Models’ Understanding of Discourse Relations</strong>” received the <strong><font color="blue">Area Chair Award</font></strong> at <a href="https://2024.aclweb.org/" target="_blank" rel="noopener noreferrer">ACL 2024</a>! Congrats to Yisong and the whole team! <img class="emoji" title=":fire:" alt=":fire:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">May, 2024</th> <td> Our paper “<strong>Discursive Socratic Questioning: Evaluating the Faithfulness of Language Models’ Understanding of Discourse Relations</strong>” and “<strong>Benchmarking Large Language Models on Communicative Medical Coaching: a Novel System and Dataset</strong>” got accepted to <a href="https://2024.aclweb.org/" target="_blank" rel="noopener noreferrer">ACL 2024</a> (one Main and one Finding)! Congrats to all co-authors! <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Oct, 2023</th> <td> Our paper “<strong>Towards Informative Few-Shot Prompt with Maximum Information Gain for In-Context Learning</strong>” got accepted to the <a href="https://2023.emnlp.org/" target="_blank" rel="noopener noreferrer">EMNLP 2023</a> (Finding)! Thanks to my advisor! </td> </tr> <tr> <th scope="row">May, 2023</th> <td> Our paper “<strong>Zero-Shot Automatic Pronunciation Assessment</strong>” got accepted to <a href="https://interspeech2023.org/" target="_blank" rel="noopener noreferrer">Interspeech 2023</a>! Thanks to all collaborators! </td> </tr> <tr> <th scope="row">Nov, 2022</th> <td> Our paper “<strong>Extrapolative Continuous-time Bayesian Neural Network for Fast Training-free Test-time Adaptation</strong>” got accepted to <a href="https://neurips.cc/Conferences/2022" target="_blank" rel="noopener noreferrer">NeurIPS 2022</a>! Congrats to all co-authors! </td> </tr> <tr> <th scope="row">May, 2022</th> <td> I passed my Ph.D. Qualifying Examination (PQE) and became a Ph.D. candidate! Many thanks to my advisor! <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">May, 2021</th> <td> Our paper “<strong>STRODE: Stochastic Boundary Ordinary Differential Equation</strong>” got accepted to <a href="https://icml.cc/Conferences/2021" target="_blank" rel="noopener noreferrer">ICML 2021</a>! Thanks to all collaborators! </td> </tr> </table> </div> </div> <div class="publications"> <h2 style="font-size:1.6rem">Selected Publications</h2> <h2 style="font-size:1.0rem">(*) denotes equal contribution</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Arxiv</abbr></div> <div id="wang2025fostering" class="col-sm-8"> <div class="title">Fostering Video Reasoning via Next-Event Prediction</div> <div class="author"> Haonan Wang*, <em>Hongfu Liu*</em>, Xiangyan Liu, Chao Du, Kawaguchi Kenji, Ye Wang, and Tianyu Pang</div> <div class="periodical"> <em>In ICML Building Physically Plausible World Models Workshop</em> 2025 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2505.22457" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/sail-sg/Video-Next-Event-Prediction" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Next-token prediction serves as the foundational learning task enabling reasoning in LLMs. But what should the learning task be when aiming to equip MLLMs with temporal reasoning capabilities over video inputs? Existing tasks such as video question answering often rely on annotations from humans or much stronger MLLMs, while video captioning tends to entangle temporal reasoning with spatial information. To address this gap, we propose next-event prediction (NEP), a learning task that harnesses future video segments as a rich, self-supervised signal to foster temporal reasoning. We segment each video into past and future frames: the MLLM takes the past frames as input and predicts a summary of events derived from the future frames, thereby encouraging the model to reason temporally in order to complete the task. To support this task, we curate V1-33K, a dataset comprising 33,000 automatically extracted video segments spanning diverse real-world scenarios. We further explore a range of video instruction-tuning strategies to study their effects on temporal reasoning. To evaluate progress, we introduce FutureBench to assess coherence in predicting unseen future events. Experiments validate that NEP offers a scalable and effective training paradigm for fostering temporal reasoning in MLLMs.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICLR</abbr></div> <div id="liu2024calibration" class="col-sm-8"> <div class="title">On Calibration of LLM-based Guard Models for Reliable Content Moderation</div> <div class="author"> <em>Hongfu Liu</em>, Hengguan Huang, Xiangming Gu, Hao Wang, and Ye Wang</div> <div class="periodical"> <em>In International Conference on Learning Representations (ICLR),</em> 2024 </div> <em>Also in NeurIPS Safe GenAI workshop, 2024 <em style="color: #0076df;"><b>(Oral)</b></em></em> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2410.10414" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/Waffle-Liu/calibration_guard_model" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Large language models (LLMs) pose significant risks due to the potential for generating harmful content or users attempting to evade guardrails. Existing studies have developed LLM-based guard models designed to moderate the input and output of threat LLMs, ensuring adherence to safety policies by blocking content that violates these protocols upon deployment. However, limited attention has been given to the reliability and calibration of such guard models. In this work, we empirically conduct comprehensive investigations of confidence calibration for 9 existing LLM-based guard models on 12 benchmarks in both user input and model output classification. Our findings reveal that current LLM-based guard models tend to 1) produce overconfident predictions, 2) exhibit significant miscalibration when subjected to jailbreak attacks, and 3) demonstrate limited robustness to the outputs generated by different types of response models. Additionally, we assess the effectiveness of post-hoc calibration methods to mitigate miscalibration. We demonstrate the efficacy of temperature scaling and, for the first time, highlight the benefits of contextual calibration for confidence calibration of guard models, particularly in the absence of validation sets. Our analysis and experiments underscore the limitations of current LLM-based guard models and provide valuable insights for the future development of well-calibrated guard models toward more reliable content moderation. We also advocate for incorporating reliability evaluation of confidence calibration when releasing future LLM-based guard models.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">EMNLP</abbr></div> <div id="liu2024advancing" class="col-sm-8"> <div class="title">Advancing Adversarial Suffix Transfer Learning on Aligned Large Language Models</div> <div class="author"> <em>Hongfu Liu*</em>, Yuxi Xie*, Ye Wang, and Michael Shieh</div> <div class="periodical"> <em>In Conference on Empirical Methods in Natural Language Processing (EMNLP),</em> 2024 </div> <em>Also in NeurIPS Red Teaming GenAI workshop, 2024 <em style="color: #0076df;"><b></b></em></em> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2408.14866" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/Waffle-Liu/DeGCG" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Language Language Models (LLMs) face safety concerns due to potential misuse by malicious users. Recent red-teaming efforts have identified adversarial suffixes capable of jailbreaking LLMs using the gradient-based search algorithm Greedy Coordinate Gradient (GCG). However, GCG struggles with computational inefficiency, limiting further investigations regarding suffix transferability and scalability across models and data. In this work, we bridge the connection between search efficiency and suffix transferability. We propose a two-stage transfer learning framework, DeGCG, which decouples the search process into behavior-agnostic pre-searching and behavior-relevant post-searching. Specifically, we employ direct first target token optimization in pre-searching to facilitate the search process. We apply our approach to cross-model, cross-data, and self-transfer scenarios. Furthermore, we introduce an interleaved variant of our approach, i-DeGCG, which iteratively leverages self-transferability to accelerate the search process. Experiments on HarmBench demonstrate the efficiency of our approach across various models and domains. Notably, our i-DeGCG outperforms the baseline on Llama2-chat-7b with ASRs of 43.9 (+22.2) and 39.0 (+19.5) on valid and test sets, respectively. Further analysis on cross-model transfer indicates the pivotal role of first target token optimization in leveraging suffix transferability for efficient searching.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">EMNLP</abbr><span class="award badge">Oral</span> </div> <div id="liu2023advancing" class="col-sm-8"> <div class="title">Advancing Test-Time Adaptation in Wild Acoustic Test Settings</div> <div class="author"> <em>Hongfu Liu</em>, Hengguan Huang, and Ye Wang</div> <div class="periodical"> <em>In Conference on Empirical Methods in Natural Language Processing (EMNLP),</em> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2310.09505" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/Waffle-Liu/CEA" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Acoustic foundation models, fine-tuned for Automatic Speech Recognition (ASR), suffer from performance degradation in wild acoustic test settings when deployed in real-world scenarios. Stabilizing online Test-Time Adaptation (TTA) under these conditions remains an open and unexplored question. Existing wild vision TTA methods often fail to handle speech data effectively due to the unique characteristics of high-entropy speech frames, which are unreliably filtered out even when containing crucial semantic content. Furthermore, unlike static vision data, speech signals follow short-term consistency, requiring specialized adaptation strategies. In this work, we propose a novel wild acoustic TTA method tailored for ASR fine-tuned acoustic foundation models. Our method, Confidence-Enhanced Adaptation, performs frame-level adaptation using a confidence-aware weight scheme to avoid filtering out essential information in high-entropy frames. Additionally, we apply consistency regularization during test-time optimization to leverage the inherent short-term consistency of speech signals. Our experiments on both synthetic and real-world datasets demonstrate that our approach outperforms existing baselines under various wild acoustic test settings, including Gaussian noise, environmental sounds, accent variations, and sung speech.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">ACL</abbr><span class="award badge">Oral</span> </div> <div id="miao2024discursive" class="col-sm-8"> <div class="title">Discursive Socratic Questioning: Evaluating the Faithfulness of Language Models’ Understanding of Discourse Relations</div> <div class="author"> Yisong Miao, <em>Hongfu Liu</em>, Wenqiang Lei, Nancy F. Chen, and Min-Yen Kan</div> <div class="periodical"> <em>In Annual Meeting of the Association for Computational Linguistics (ACL),</em> 2024 </div> <span class="honor"> Area Chair Award </span> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2024.acl-long.341/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/YisongMiao/DiSQ-Score" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>While large language models have significantly enhanced the effectiveness of discourse relation classifications, it remains unclear whether their comprehension is faithful and reliable. We provide DiSQ, a new method for evaluating the faithfulness of understanding discourse based on question answering. We first employ in-context learning to annotate the reasoning for discourse comprehension, based on the connections among key events within the discourse. Following this, DiSQ interrogates the model with a sequence of questions to assess its grasp of core event relations, its resilience to counterfactual queries, as well as its consistency to its previous responses. then evaluate language models with different architectural designs using DiSQ, finding: (1) DiSQ presents a significant challenge for all models, with the top-performing GPT model attaining only 41% of the ideal performance in PDTB; (2) DiSQ is robust to domain shifts and paraphrase variations; (3) Open-source models generally lag behind their closed-source GPT counterparts, with notable exceptions being those enhanced with chat and code/math features; (4) Our analysis validates the effectiveness of explicitly signalled discourse connectives, the role of contextual information, and the benefits of using historical QA data.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ACL Findings</abbr></div> <div id="huang2024benchmarking" class="col-sm-8"> <div class="title">Benchmarking Large Language Models on Communicative Medical Coaching: a Novel System and Dataset</div> <div class="author"> Hengguan Huang*, Songtao Wang*, <em>Hongfu Liu</em>, Hao Wang, and Ye Wang</div> <div class="periodical"> <em>In Findings of Annual Meeting of the Association for Computational Linguistics (ACL),</em> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2402.05547" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/zerowst/Chatcoach" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Traditional applications of natural language processing (NLP) in healthcare have predominantly focused on patient-centered services, enhancing patient interactions and care delivery, such as through medical dialogue systems. However, the potential of NLP to benefit inexperienced doctors, particularly in areas such as communicative medical coaching, remains largely unexplored. We introduce "ChatCoach", a human-AI cooperative framework designed to assist medical learners in practicing their communication skills during patient consultations. ChatCoach differentiates itself from conventional dialogue systems by offering a simulated environment where medical learners can practice dialogues with a patient agent, while a coach agent provides immediate, structured feedback. This is facilitated by our proposed Generalized Chain-of-Thought (GCoT) approach, which fosters the generation of structured feedback and enhances the utilization of external knowledge sources. Additionally, we have developed a dataset specifically for evaluating Large Language Models (LLMs) within the ChatCoach framework on communicative medical coaching tasks. Our empirical results validate the effectiveness of ChatCoach.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">EMNLP Findings</abbr></div> <div id="liu2023towards" class="col-sm-8"> <div class="title">Towards Informative Few-Shot Prompt with Maximum Information Gain for In-Context Learning</div> <div class="author"> <em>Hongfu Liu</em>, and Ye Wang</div> <div class="periodical"> <em>In Findings of Conference on Empirical Methods in Natural Language Processing (EMNLP),</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2310.08923" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://waffle-liu.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Large Language models (LLMs) possess the capability to engage In-context Learning (ICL) by leveraging a few demonstrations pertaining to a new downstream task as conditions. However, this particular learning paradigm suffers from high instability stemming from substantial variances induced by factors such as the input distribution of selected examples, their ordering, and prompt formats. In this work, we demonstrate that even when all these factors are held constant, the random selection of examples still results in high variance. Consequently, we aim to explore the informative ability of data examples by quantifying the Information Gain (IG) obtained in prediction after observing a given example candidate. Then we propose to sample those with maximum IG. Additionally, we identify the presence of template bias, which can lead to unfair evaluations of IG during the sampling process. To mitigate this bias, we introduce Calibration Before Sampling strategy. The experimental results illustrate that our proposed method can yield an average relative improvement of 14.3% across six classification tasks using three LLMs.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS</abbr></div> <div id="ecbnn" class="col-sm-8"> <div class="title">Extrapolative Continuous-time Bayesian Neural Network for Fast Training-free Test-time Adaptation</div> <div class="author"> Hengguan Huang, Xiangming Gu, Hao Wang, Chang Xiao, <em>Hongfu Liu</em>, and Ye Wang</div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems (NeurIPS),</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/e9e1a0abc1a5b19a4aeb80dab19c82ae-Paper-Conference.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/guxm2021/ECBNN" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Human intelligence has shown remarkably lower latency and higher precision than most AI systems when processing non-stationary streaming data in real-time. Numerous neuroscience studies suggest that such abilities may be driven by internal predictive modeling. In this paper, we explore the possibility of introducing such a mechanism in unsupervised domain adaptation (UDA) for handling non-stationary streaming data for real-time streaming applications. We propose to formulate internal predictive modeling as a continuous-time Bayesian filtering problem within a stochastic dynamical system context. Such a dynamical system describes the dynamics of model parameters of a UDA model evolving with non-stationary streaming data. Building on such a dynamical system, we then develop extrapolative continuous-time Bayesian neural networks (ECBNN), which generalize existing Bayesian neural networks to represent temporal dynamics and allow us to extrapolate the distribution of model parameters before observing the incoming data, therefore effectively reducing the latency. Remarkably, our empirical results show that ECBNN is capable of continuously generating better distributions of model parameters along the time axis given historical data only, thereby achieving (1) training-free test-time adaptation with low latency, (2) gradually improved alignment between the source and target features and (3) gradually improved model performance over time during the real-time testing stage.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICML</abbr></div> <div id="huang2021strode" class="col-sm-8"> <div class="title">STRODE: Stochastic Boundary Ordinary Differential Equation</div> <div class="author"> Hengguan Huang, <em>Hongfu Liu</em>, Hao Wang, Chang Xiao, and Ye Wang</div> <div class="periodical"> <em>In International Conference on Machine Learning (ICML),</em> 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2107.08273" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/Waffle-Liu/STRODE" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Perception of time from sequentially acquired sensory inputs is rooted in everyday behaviors of individual organisms. Yet, most algorithms for time-series modeling fail to learn dynamics of random event timings directly from visual or audio inputs, requiring timing annotations during training that are usually unavailable for real-world applications. For instance, neuroscience perspectives on postdiction imply that there exist variable temporal ranges within which the incoming sensory inputs can affect the earlier perception, but such temporal ranges are mostly unannotated for real applications such as automatic speech recognition (ASR). In this paper, we present a probabilistic ordinary differential equation (ODE), called STochastic boundaRy ODE (STRODE), that learns both the timings and the dynamics of time series data without requiring any timing annotations during training. STRODE allows the usage of differential equations to sample from the posterior point processes, efficiently and analytically. We further provide theoretical guarantees on the learning of STRODE. Our empirical results show that our approach successfully infers event timings of time series data. Our method achieves competitive or superior performances compared to existing state-of-the-art methods for both synthetic and real-world datasets.</p> </div> </div> </div> </li> </ol> </div> <br> <br> <div class="news"> <h2 style="font-size:1.6rem">Academic Services</h2> <div class="table-responsive"> <ul> <li> <p>Conference Reviewers:</p> <p>ACL Rolling Review (2023&amp;2024&amp;2025), NeurIPS (2024&amp;2025), ICLR (2024&amp;2025), IJCAI (2024)</p> </li> </ul> </div> </div> <br> <br> <div class="news"> <h2 style="font-size:1.6rem">Teaching</h2> <div class="table-responsive"> <ul> <li> <p>CS4347: Sound and Music Computing (2022 Spring / 2022 Fall)</p> </li> </ul> <ul> <li> <p>CS5242: Neural Networks and Deep Learning (2021 Fall / 2023 Spring)</p> </li> </ul> </div> </div> <br> <br> <div class="news"> <h2 style="font-size:1.6rem">MISC</h2> <div class="table-responsive"> <ul> <li> <p>I am a big fan of live music and attend numerous concerts every year, enjoying a wide variety of music genres. My favorate genre is R&amp;B. <img class="emoji" title=":musical_note:" alt=":musical_note:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3b5.png" height="20" width="20"> <img class="emoji" title=":notes:" alt=":notes:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3b6.png" height="20" width="20"> <img class="emoji" title=":musical_score:" alt=":musical_score:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3bc.png" height="20" width="20"> <img class="emoji" title=":musical_keyboard:" alt=":musical_keyboard:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3b9.png" height="20" width="20"></p> </li> </ul> <ul> <li> <p>I also enjoy traveling and exploring different cultures, especially experiencing diverse cuisines from around the world.</p> </li> </ul> </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Hongfu Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>